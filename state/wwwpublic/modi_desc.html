<script>
function show_faq() {
    /* UI V3 uses toggle_info to show inline FAQ while V2 opens front page */
    if (typeof toggle_info === 'function') {
        toggle_info('supportInfo');
    } else {
        window.location.href='http://erda.ku.dk';
    }
}

</script>
<p>
MPI Oriented Development and Investigation or MODI provides a cluster of 8
compute nodes that can be utilized for long running batch jobs.<br/>
Each node offers dual AMD EPYC 32-core CPUs and 256GB of memory. Nodes are
connected with a 25Gbit network providing RoCE for efficient communication.<br/>
The system is configured as a SLURM cluster so that regular SLURM commands like
<code>sbatch</code>, <code>squeue</code>, <code>sinfo</code> and friends should
be used to schedule and monitor your programs running as batch jobs.
<br/><br/>

The current setup does not at the moment support interactive functionalities
such as <code>srun</code> and similar.<br/><br/>

Please refer to the separate 
<a href="/public/MODI-user-guide.pdf" target=_blank>MODI user guide</a> for
details about using MODI.<br/>
As additional help to getting started, MODI provides the <a href="https://modi-helper-scripts.readthedocs.io/en/latest/" target=_blank>MODI Helper Scripts</a>
that makes it easiser to manage environment and submitting jobs without requiring too much knowledge about the system or SLURM in general.
<br/>
<br/>
<span class="warningtext">NOTE</span>: it's particularly important to pay
 attention to the part about running <code>sbatch</code> commands using the
shared <code>modi_mount</code> directory for jobs to succeed.
</p>
